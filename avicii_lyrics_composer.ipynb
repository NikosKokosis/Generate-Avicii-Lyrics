{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Activation\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"feeling my way through the darkness guided by a beating heart i can't tell where the journey will end but i know where to start\\n\\nthey tell me i'm too young to understand they say i'm caught up in a dream well life will pass me by if i don't open up m\""
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = 'avicii_lyrics.txt'\n",
    "with io.open(source, 'r') as corpus:\n",
    "# Read the contents of the file and convert the text to lowercase\n",
    "    corpus = corpus.read().lower()\n",
    "\n",
    "corpus[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"feeling my way through the darkness guided by a beating heart i can't tell where the journey will end but i know where to start\\n\\nthey tell me i'm too young to understand they say i'm caught up in a dream well life will pass me by if i don't open up m\""
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = corpus.replace('â€™',' a')\n",
    "corpus = corpus.replace(';','?')\n",
    "symbols_to_remove = ['¶', '˜','¦', 'â', 'ã', '€','¤','™']\n",
    "\n",
    "for symbol in symbols_to_remove:\n",
    "    corpus = corpus.replace(symbol, '')\n",
    "\n",
    "corpus[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the Unique characters from all text corpus\n",
    "chars = sorted(list(set(corpus)))\n",
    "chars_lenght = len(chars)\n",
    "chars_lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " '?',\n",
       " '[',\n",
       " ']',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '”']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-weight:bold; font-size:larger;\">**Mapping Characters to Indexes and Versa for Better Training**</span><br><br>\n",
    "**When we work with models that analyze text, such as an LSTM model, we need a method to change the text into numbers. This helps the model understand and work with the text.**<br><br>\n",
    "<span style='color:green'>**We use a process called mapping**</span> **, which involves assigning unique numbers to each character in the text. This mapping allows the model to learn patterns and relationships in the text.**<br>\n",
    "<span style='color:green'>**Mapping characters to numbers also helps with efficient indexing and searching**</span>**. In LSTM models, we often need to find and change specific characters in the text. By creating dictionaries that connect characters with their corresponding numbers, we can quickly find the number for a character or retrieve the character for a given number. This saves time and avoids searching through the entire text repeatedly.**<br>\n",
    "<span style='color:green'>**The dictionaries we create also help us manage the vocabulary of the model**</span>**. By mapping characters to numbers, we establish a consistent way to represent the vocabulary. This makes it easier to add or change words in the vocabulary when necessary. The dictionaries keep track of which number corresponds to which character, making it simpler to handle the model's vocabulary.**<br><br>\n",
    "**In summary, by creating dictionaries that map characters to numbers, we provide a useful way to represent, process, and change text data in models like LSTM. These dictionaries are important for converting text into numbers, efficient indexing and searching, and effective management of the model's vocabulary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '\"': 3,\n",
       " \"'\": 4,\n",
       " '(': 5,\n",
       " ')': 6,\n",
       " ',': 7,\n",
       " '-': 8,\n",
       " '.': 9,\n",
       " '0': 10,\n",
       " '1': 11,\n",
       " '2': 12,\n",
       " '3': 13,\n",
       " '4': 14,\n",
       " '5': 15,\n",
       " '6': 16,\n",
       " '7': 17,\n",
       " '8': 18,\n",
       " '9': 19,\n",
       " ':': 20,\n",
       " '?': 21,\n",
       " '[': 22,\n",
       " ']': 23,\n",
       " 'a': 24,\n",
       " 'b': 25,\n",
       " 'c': 26,\n",
       " 'd': 27,\n",
       " 'e': 28,\n",
       " 'f': 29,\n",
       " 'g': 30,\n",
       " 'h': 31,\n",
       " 'i': 32,\n",
       " 'j': 33,\n",
       " 'k': 34,\n",
       " 'l': 35,\n",
       " 'm': 36,\n",
       " 'n': 37,\n",
       " 'o': 38,\n",
       " 'p': 39,\n",
       " 'q': 40,\n",
       " 'r': 41,\n",
       " 's': 42,\n",
       " 't': 43,\n",
       " 'u': 44,\n",
       " 'v': 45,\n",
       " 'w': 46,\n",
       " 'x': 47,\n",
       " 'y': 48,\n",
       " 'z': 49,\n",
       " '”': 50}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create dictionaries mapping characters to indices and versa.\n",
    "\n",
    "Args:\n",
    "    chars (list): List of unique characters.\n",
    "\n",
    "Returns:\n",
    "    dict: Dictionary mapping characters to indexes.\n",
    "    dict: Dictionary mapping indexes to characters.\n",
    "\"\"\"\n",
    "def create_dictionaries(chars):\n",
    "\n",
    "    return dict((c, i) for i, c in enumerate(chars)), dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "char_to_index, index_to_char =  create_dictionaries(chars)\n",
    "char_to_index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-weight:bold; font-size:larger;\">**Prepare the Data for the LSTM model**</span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Characters in windows:\n",
      " [\"feeling my way through the darkness guided by a beating heart i can't tell where the journey will end but i know where t\", \"eeling my way through the darkness guided by a beating heart i can't tell where the journey will end but i know where to\", \"eling my way through the darkness guided by a beating heart i can't tell where the journey will end but i know where to \", \"ling my way through the darkness guided by a beating heart i can't tell where the journey will end but i know where to s\", \"ing my way through the darkness guided by a beating heart i can't tell where the journey will end but i know where to st\", \"ng my way through the darkness guided by a beating heart i can't tell where the journey will end but i know where to sta\", \"g my way through the darkness guided by a beating heart i can't tell where the journey will end but i know where to star\", \" my way through the darkness guided by a beating heart i can't tell where the journey will end but i know where to start\", \"my way through the darkness guided by a beating heart i can't tell where the journey will end but i know where to start\\n\", \"y way through the darkness guided by a beating heart i can't tell where the journey will end but i know where to start\\n\\n\", \" way through the darkness guided by a beating heart i can't tell where the journey will end but i know where to start\\n\\nt\", \"way through the darkness guided by a beating heart i can't tell where the journey will end but i know where to start\\n\\nth\", \"ay through the darkness guided by a beating heart i can't tell where the journey will end but i know where to start\\n\\nthe\", \"y through the darkness guided by a beating heart i can't tell where the journey will end but i know where to start\\n\\nthey\", \" through the darkness guided by a beating heart i can't tell where the journey will end but i know where to start\\n\\nthey \"] \n",
      "\n",
      "\n",
      "Next Character foreach window:\n",
      " ['o', ' ', 's', 't', 'a', 'r', 't', '\\n', '\\n', 't', 'h', 'e', 'y', ' ', 't']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create sequences and labels from the given text.\n",
    "\n",
    "Args:\n",
    "    corpus (str): The input text corpus.\n",
    "    window_size (int): Length of each sequence.\n",
    "    step (int): Number of steps between each sequence.\n",
    "\n",
    "Returns:\n",
    "    list: List of sequences (windows).\n",
    "    list: List of labels.\n",
    "\"\"\"\n",
    "def create_sequences(corpus, window_size, step):\n",
    "\n",
    "    windows = []\n",
    "    labels = []\n",
    "    # Generate sequences and labels\n",
    "    for i in range(0, len(corpus) - window_size, step):\n",
    "        windows.append(corpus[i: i + window_size])\n",
    "        labels.append(corpus[i + window_size])\n",
    "\n",
    "    return windows, labels\n",
    "\n",
    "window_size = 120\n",
    "step = 1\n",
    "\n",
    "values, labels = create_sequences(corpus, window_size, step)\n",
    "print('\\nCharacters in windows:\\n', values[:15], '\\n') # Characters of window \n",
    "print('\\nNext Character foreach window:\\n', labels[:15]) # Next character of window"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-weight:bold; font-size:larger;\">**One Hot Encode: Convert sequences and labels to one-hot arrays**</span><br><br>\n",
    "**When we convert sequences and labels into one-hot arrays, we create a useful numerical format for the LSTM model. This format helps the model understand and learn from the data we provide, including both the input sequences and the target labels. In one-hot arrays, each character or label is represented as a binary vector. Each element in the vector tells us whether a particular character or label is present or not. This binary representation helps the model process and make sense of the data effectively.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((71933, 120, 51), (71933, 51))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Convert sequences and labels to one-hot arrays.\n",
    "\n",
    "Args:\n",
    "    sequences (list): List of sequences of characters (values).\n",
    "    window_size (int): Length of each sequence.\n",
    "    chars_length (int): Length of the array of unique characters.\n",
    "    char_to_index (list): A dictionary list mapping unique characters to their indices.\n",
    "    labels (list): List of labels.\n",
    "\n",
    "Returns:\n",
    "    np.ndarray: Array of sequences encoded as one-hot arrays.\n",
    "    np.ndarray: Array of labels encoded as one-hot arrays.\n",
    "\n",
    "A One-Hot array is represented in the following form:\n",
    "\n",
    "                      a b c d.....\n",
    "                     [1 0 0 0.....]\n",
    "                     [0 1 0 0.....]\n",
    "                     [0 0 1 0.....]\n",
    "\"\"\"\n",
    "def convert_to_one_hot_arrays(sequences, window_size, chars_length, char_to_index, labels):\n",
    "    \n",
    "    # Create empty arrays of appropriate shape for sequences (x) and labels (y)\n",
    "    x = np.zeros((len(sequences), window_size, chars_length), dtype=np.bool)\n",
    "    y = np.zeros((len(sequences), chars_length), dtype=np.bool)\n",
    "\n",
    "    # Iterate over each sequence and its corresponding label\n",
    "    for i, sentence in enumerate(sequences):\n",
    "        # Encode each character in the sequence as a one-hot vector\n",
    "        for j, char in enumerate(sentence):\n",
    "            x[i, j, char_to_index[char]] = 1\n",
    "        # Encode the label as a one-hot vector\n",
    "        y[i, char_to_index[labels[i]]] = 1\n",
    "\n",
    "    return x, y\n",
    "\n",
    "X, y = convert_to_one_hot_arrays(values, window_size, chars_lenght, char_to_index, labels)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-weight:bold; font-size:larger;\">**Create the model**</span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 1024)              4407296   \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                65600     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 51)                3315      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 51)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,476,211\n",
      "Trainable params: 4,476,211\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# LSTM input shape: [samples, time steps, features]\n",
    "model.add(LSTM(1024, input_shape=(window_size, chars_lenght)))\n",
    "# Dense layer to capture patterns in the text data\n",
    "# We put the Dense layer to train our model with some patterns we have in text like every time we have ',' the next character is ' '\n",
    "model.add(Dense(64))\n",
    "# Output layer with the same number of units as the length of the unique characters\n",
    "model.add(Dense(chars_lenght))\n",
    "# Use softmax activation because it is a non-linear function and we are using a categorical loss function\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Compile the model using RMSprop optimizer and categorical cross-entropy loss\n",
    "# RMSprop is a typical optimazer for categorical crossentropy problems\n",
    "model.compile(optimizer='RMSprop', loss='categorical_crossentropy')\n",
    "\n",
    "# Print the summary of the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "141/141 [==============================] - 2763s 20s/step - loss: 3.1532\n",
      "Epoch 2/25\n",
      "141/141 [==============================] - 2711s 19s/step - loss: 2.3814\n",
      "Epoch 3/25\n",
      "141/141 [==============================] - 2690s 19s/step - loss: 2.0429\n",
      "Epoch 4/25\n",
      "141/141 [==============================] - 2681s 19s/step - loss: 1.7651\n",
      "Epoch 5/25\n",
      "141/141 [==============================] - 2710s 19s/step - loss: 1.5059\n",
      "Epoch 6/25\n",
      "141/141 [==============================] - 2682s 19s/step - loss: 1.2497\n",
      "Epoch 7/25\n",
      "141/141 [==============================] - 2675s 19s/step - loss: 1.0081\n",
      "Epoch 8/25\n",
      "141/141 [==============================] - 2679s 19s/step - loss: 0.7941\n",
      "Epoch 9/25\n",
      "141/141 [==============================] - 2676s 19s/step - loss: 0.6087\n",
      "Epoch 10/25\n",
      "141/141 [==============================] - 2677s 19s/step - loss: 0.4566\n",
      "Epoch 11/25\n",
      "141/141 [==============================] - 2706s 19s/step - loss: 0.3352\n",
      "Epoch 12/25\n",
      "141/141 [==============================] - 2675s 19s/step - loss: 0.2470\n",
      "Epoch 13/25\n",
      "141/141 [==============================] - 2729s 19s/step - loss: 0.1841\n",
      "Epoch 14/25\n",
      "141/141 [==============================] - 2869s 20s/step - loss: 0.1488\n",
      "Epoch 15/25\n",
      "141/141 [==============================] - 2709s 19s/step - loss: 0.1269\n",
      "Epoch 16/25\n",
      "141/141 [==============================] - 2740s 19s/step - loss: 0.1114\n",
      "Epoch 17/25\n",
      "141/141 [==============================] - 2896s 20s/step - loss: 0.1007\n",
      "Epoch 18/25\n",
      "141/141 [==============================] - 2897s 21s/step - loss: 0.0927\n",
      "Epoch 19/25\n",
      "141/141 [==============================] - 2831s 20s/step - loss: 0.0870\n",
      "Epoch 20/25\n",
      "141/141 [==============================] - 2778s 20s/step - loss: 0.0812\n",
      "Epoch 21/25\n",
      "141/141 [==============================] - 2817s 20s/step - loss: 0.0741\n",
      "Epoch 22/25\n",
      "141/141 [==============================] - 2803s 20s/step - loss: 0.0722\n",
      "Epoch 23/25\n",
      "141/141 [==============================] - 2787s 20s/step - loss: 0.0668\n",
      "Epoch 24/25\n",
      "141/141 [==============================] - 2779s 20s/step - loss: 0.0647\n",
      "Epoch 25/25\n",
      "141/141 [==============================] - 2936s 21s/step - loss: 0.0618\n"
     ]
    }
   ],
   "source": [
    "# Fit the model to the data\n",
    "model.fit(X, y, batch_size=512, epochs=25)\n",
    "\n",
    "# Save the model weights to a file\n",
    "model.save_weights('model_weights_25epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate text based on a given model and initial text.\n",
    "\n",
    "Args:\n",
    "    model (Sequential): The trained LSTM model.\n",
    "    text (str): The initial text to start generating from.\n",
    "    window_size (int): The size of the window for generating sequences.\n",
    "    chars_length (int): The length of the array of unique characters.\n",
    "    char_to_index (dict): A dictionary mapping unique characters to their indices.\n",
    "    index_to_char (dict): A dictionary mapping indices to their corresponding characters.\n",
    "\n",
    "Returns:\n",
    "    str: The generated text.\n",
    "\"\"\"\n",
    "\n",
    "def generate_text(model, text, window_size, chars_length, char_to_index, index_to_char):\n",
    "    \n",
    "    # Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "    # Print the initial text\n",
    "    sys.stdout.write(text)\n",
    "\n",
    "    # Generate the next 200 characters based on the text\n",
    "    for i in range(200):\n",
    "        X = np.zeros((1, window_size, chars_length))\n",
    "\n",
    "        # Convert the characters in the text to one-hot arrays\n",
    "        for j, character in enumerate(text):\n",
    "            X[0, j, char_to_index[character]] = 1.\n",
    "\n",
    "        # Predict the next character based on the one-hot encoded seed\n",
    "        predictions = model.predict(X, verbose=0)[0]\n",
    "        index = np.argmax(predictions)\n",
    "\n",
    "        # Convert the predicted index back to the corresponding character\n",
    "        next_char = index_to_char[index]\n",
    "\n",
    "        # Update the seed by removing the first character and appending the next character\n",
    "        text = text[1:] + next_char\n",
    "\n",
    "        # Print the next character\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The LSTM model was Trained for 12 Epochs | Taking 9.4 hours to Run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " they tell me i'm too young to understand they say i'm caught up in a dream well life will pass me by if i don't open up my eyes\n",
      "well that's fine by me\n",
      "\n",
      "\n",
      "st wakh me hemp fightrenter\n",
      "\n",
      "all the ting to the with you ball rang\n",
      "these are the days we won't rogher\n",
      "tell me ahay you see love i wanna love ya,\n",
      "i wanna love ya, lik"
     ]
    }
   ],
   "source": [
    "text=\" They tell me I'm too young to understand They say I'm caught up in a dream Well life will pass me by if I don't open up\"\n",
    "\n",
    "generate_text(model, text, window_size, chars_lenght, char_to_index, index_to_char)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The LSTM model was Trained for 12 Epochs | Taking 19 hours to Run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " they tell me i'm too young to understand they say i'm caught up in a dream well life will pass me by if i don't open up my eyes\n",
      "well that's fine by me\n",
      "\n",
      "\n",
      "so wake me up when it's all over\n",
      "when i'm wiser and i'm older\n",
      "all this time i was finding myself and i dill on your love and it has it all just fades away\n",
      "it all just"
     ]
    }
   ],
   "source": [
    "text=\" They tell me I'm too young to understand They say I'm caught up in a dream Well life will pass me by if I don't open up\"\n",
    "\n",
    "generate_text(model, text, window_size, chars_lenght, char_to_index, index_to_char)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When comparing the same model trained for different epochs, the LSTM model trained for 25 epochs showed better results. However, it still couldn't completely close the difference of all 150 letters between the predicted and actual text. In conlution, even though the LSTM model improved a lot, it couldn't exactly match the expected and observed lyrics for the whole sequence of 200 letters.**<br><br>\n",
    "**The lyrics is below**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Screenshot_4.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
